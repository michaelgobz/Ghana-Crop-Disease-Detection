#from ultralytics.utils.callbacks.base import on_predict_end
# Check GPU type
!nvidia-smi


# Install ultralytics
!pip -q install  ultralytics


# Import libraries
import pandas as pd
import os
from pathlib import Path
import shutil
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tqdm.notebook import tqdm
import cv2
import yaml
import matplotlib.pyplot as plt
from ultralytics import YOLO
import multiprocessing



# get data from s3

# Create directories in standard python environment
os.makedirs('./dataset', exist_ok=True)
os.makedirs('./images', exist_ok=True)
os.makedirs('./data', exist_ok=True)

# download the data
!aws s3 cp s3://crop-diseases/images.zip ./data
!aws s3 cp s3://crop-diseases/Train.csv ./data
!aws s3 cp s3://crop-diseases/Test.csv ./data
!aws s3 cp s3://crop-diseases/SampleSubmission.csv ./data



# Set up directories for training a yolo model

DATA_DIR = Path('data')

# Images directories
DATASET_DIR = Path('dataset')
IMAGES_DIR = DATASET_DIR / 'images'
TRAIN_IMAGES_DIR = IMAGES_DIR / 'train'
VAL_IMAGES_DIR = IMAGES_DIR / 'val'
TEST_IMAGES_DIR = IMAGES_DIR / 'test'

# Labels directories
LABELS_DIR = DATASET_DIR / 'labels'
TRAIN_LABELS_DIR = LABELS_DIR / 'train'
VAL_LABELS_DIR = LABELS_DIR / 'val'
TEST_LABELS_DIR = LABELS_DIR / 'test'


# Unzip images to 'images' dir
shutil.unpack_archive(DATA_DIR / 'images.zip', 'images')


# Load train and test files
train = pd.read_csv(DATA_DIR / 'Train.csv')
test = pd.read_csv(DATA_DIR / 'Test.csv')
ss = pd.read_csv(DATA_DIR / 'SampleSubmission.csv')

# Add an image_path column
train['image_path'] = [Path('images/' + x) for x in train.Image_ID]
test['image_path'] = [Path('images/' + x) for x in test.Image_ID]

# Map str classes to ints (label encoding targets)
class_mapper = {x:y for x,y in zip(sorted(train['class'].unique().tolist()), range(train['class'].nunique()))}
train['class_id'] = train['class'].map(class_mapper)

# Preview the head of the train set
train.head()


test.head()


ss.head()


# create more images from the transforms to balance out the other classes

import os
import numpy as np
import pandas as pd
from PIL import Image
import torchvision.transforms as T
from torchvision.transforms.functional import hflip, rotate
import random

# Define the transformation pipeline
transform = T.Compose([
    T.RandomHorizontalFlip(p=0.5),
    T.RandomRotation(60),
    T.ToTensor()
])

# Helper function to transform bounding boxes
def transform_bounding_box(bbox, image_size, transform_fn):
    """
    Transform a bounding box according to the given transformation function.

    Args:
        bbox (dict): Bounding box with keys ['xmin', 'ymin', 'xmax', 'ymax'].
        image_size (tuple): Size of the image (width, height).
        transform_fn (callable): A function that applies the same transformation to coordinates.

    Returns:
        dict: Transformed bounding box.
    """
    # Extract coordinates
    xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']
    width, height = image_size

    # Define corner points of the bounding box
    corners = np.array([
        [xmin, ymin],
        [xmax, ymin],
        [xmax, ymax],
        [xmin, ymax]
    ])

    # Apply transformation to each corner
    transformed_corners = np.array([transform_fn(pt, width, height) for pt in corners])

    # Recalculate bounding box
    x_coords = transformed_corners[:, 0]
    y_coords = transformed_corners[:, 1]
    transformed_bbox = {
        'xmin': min(x_coords),
        'ymin': min(y_coords),
        'xmax': max(x_coords),
        'ymax': max(y_coords)
    }
    return transformed_bbox

# Define specific coordinate transformation functions
def horizontal_flip(pt, width, _):
    """Flip a point horizontally."""
    return [width - pt[0], pt[1]]

def rotation(pt, width, height, angle):
    """Rotate a point around the image center."""
    cx, cy = width / 2, height / 2
    x, y = pt[0] - cx, pt[1] - cy
    rad = np.radians(angle)
    cos_a, sin_a = np.cos(rad), np.sin(rad)
    x_rot = cos_a * x - sin_a * y + cx
    y_rot = sin_a * x + cos_a * y + cy
    return [x_rot, y_rot]

# Function to apply transformations and save images
def save_transformed_images(train_set, output_dir, num_images=2):
    """
    Apply transformations to an image multiple times and save each result as a new image.

    Args:
        train_set (DataFrame): Training dataset with image paths and bounding boxes.
        output_dir (str): Directory where transformed images will be saved.
        num_images (int): Number of transformed images to generate per original image.

    Returns:
        DataFrame: Updated training dataset with new images.
    """
    os.makedirs(output_dir, exist_ok=True)
    new_train_data = train_set.copy()

    for index, row in train_set.iterrows():
        data = row.to_dict()
        original_image = Image.open(data['image_path']).convert("RGB")
        image_size = original_image.size  # (width, height)

        for _ in range(num_images):
            # Randomly choose a transformation
            transformed_image = original_image.copy()
            bbox = {
                'xmin': data['xmin'],
                'ymin': data['ymin'],
                'xmax': data['xmax'],
                'ymax': data['ymax']
            }
            transformed_bbox = bbox.copy()

            # Apply Horizontal Flip
            if random.random() < 0.5:
                transformed_image = hflip(transformed_image)
                transformed_bbox = transform_bounding_box(bbox, image_size, horizontal_flip)

            # Apply Rotation
            angle = random.uniform(-60, 60)
            transformed_image = rotate(transformed_image, angle)
            transformed_bbox = transform_bounding_box(bbox, image_size, lambda pt, w, h: rotation(pt, w, h, angle))

            # Save the transformed image
            file_name = os.path.splitext(os.path.basename(data['image_path']))[0]
            new_file_name = f"{file_name}_{np.random.randint(1, 1000)}.jpg"
            output_path = os.path.join(output_dir, new_file_name)
            transformed_image.save(output_path)

            # Append the new data
            new_train_data = pd.concat([
                new_train_data,
                pd.DataFrame({
                    'Image_ID': [new_file_name],
                    'class': [data['class']],
                    'confidence': [data['confidence']],
                    'ymin': [transformed_bbox['ymin']],
                    'xmin': [transformed_bbox['xmin']],
                    'ymax': [transformed_bbox['ymax']],
                    'xmax': [transformed_bbox['xmax']],
                    'image_path': [output_path],
                    'class_id': [data['class_id']]
                })
            ], ignore_index=True)

            print(f"Saved: {output_path}")

    return new_train_data



# Preview target distribution, seems there a class imbalance that needs to be handled
# Get normalized value counts and convert to DataFrame
train_class_distribution = train['class'].value_counts(normalize=True).reset_index()
train_class_distribution.columns = ['class', 'proportion']

# Combine both into a single DataFrame for comparison
class_distribution_df = pd.DataFrame(
    train_class_distribution,
)

# Fill NaN values with 0 (in case a class is missing in train or val)
class_distribution_df.fillna(0, inplace=True)
class_distribution_df.head()




# low represented classes

# Filter classes with proportion < 0.10 in either train or validation set
low_proportion_classes = class_distribution_df[
    (class_distribution_df['proportion'] < 0.10)
    ]

# Display the result
print(list(low_proportion_classes['class']))



# save them in a new train data set
combined_data = None
new_train_data_set = None
raw_image_data = train[train['class'].isin(list(low_proportion_classes['class']))]
new_train_data_set = save_transformed_images(raw_image_data, 'images')
combined_data = pd.concat([raw_image_data, new_train_data_set])

combined_data.head()


# shape of the new train dataset
combined_data.shape


# obtained the fairly represented

# Filter classes with proportion < 0.10 in either train or validation set
high_proportion_classes = class_distribution_df[
    (class_distribution_df['proportion'] > 0.10)
    ]

# Display the result
print(list(high_proportion_classes['class']))


# get the high proportion classes
high_prop_image_data = train[train['class'].isin(list(high_proportion_classes['class']))]
high_prop_image_data.shape


# combine the train data
transformed_data = pd.concat([high_prop_image_data, combined_data])
transformed_data.shape



# classe representation
transformed_data['class'].value_counts(normalize=True).reset_index()


# save the transformed data frame
transformed_data.to_csv('./data/tranformed_train_data.csv', index=False)


# Split data into training and validation
new_train_data = pd.read_csv('./data/tranformed_train_data.csv')
train_unique_imgs_df = train.drop_duplicates(subset = ['Image_ID'], ignore_index = True)
X_train, X_val = train_test_split(train_unique_imgs_df, test_size = 0.25, stratify=train_unique_imgs_df['class'], random_state=42)

X_train = train[train.Image_ID.isin(X_train.Image_ID)]
X_val = train[train.Image_ID.isin(X_val.Image_ID)]

# Check shapes of training and validation data
X_train.shape, X_val.shape


# Check if dirs exist, if they do, remove them, otherwise create them.
# This only needs to run once
for DIR in [TRAIN_IMAGES_DIR,VAL_IMAGES_DIR, TEST_IMAGES_DIR, TRAIN_LABELS_DIR,VAL_LABELS_DIR,TEST_LABELS_DIR]:
  if DIR.exists():
    shutil.rmtree(DIR)
  DIR.mkdir(parents=True, exist_ok = True)


# Copy train, val and test images to their respective dirs
for img in tqdm(X_train.image_path.unique()):
  shutil.copy(img, TRAIN_IMAGES_DIR / Path(img).parts[-1])

for img in tqdm(X_val.image_path.unique()):
  shutil.copy(img, VAL_IMAGES_DIR / Path(img).parts[-1])

for img in tqdm(test.image_path.unique()):
  shutil.copy(img, TEST_IMAGES_DIR / Path(img).parts[-1])


X_train.head()


import multiprocessing
from pathlib import Path
import numpy as np
from PIL import Image
from tqdm import tqdm
import shutil
import pandas as pd

# Function to convert the bboxes to YOLO format
def convert_to_yolo(bbox, width, height):
    ymin, xmin, ymax, xmax = bbox['ymin'], bbox['xmin'], bbox['ymax'], bbox['xmax']
    class_id = bbox['class_id']

    # Normalize the coordinates
    x_center = (xmin + xmax) / 2 / width
    y_center = (ymin + ymax) / 2 / height
    bbox_width = (xmax - xmin) / width
    bbox_height = (ymax - ymin) / height

    return f"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}"

# Top-level function to save annotations for a single image
def save_yolo_annotations_task(task):
    image_path, bboxes, output_dir = task
    try:
        img = np.array(Image.open(str(image_path)))
        height, width, _ = img.shape
    except Exception as e:
        print(f"Error opening image {image_path}: {e}")
        return

    label_file = Path(output_dir) / f"{Path(image_path).stem}.txt"
    with open(label_file, 'w') as f:
        for bbox in bboxes:
            annotation = convert_to_yolo(bbox, width, height)
            f.write(f"{annotation}\n")

# Function to clear output directory
def clear_output_dir(output_dir):
    if Path(output_dir).exists():
        shutil.rmtree(output_dir)
    Path(output_dir).mkdir(parents=True, exist_ok=True)

# Function to process the dataset and save annotations
def process_dataset(dataframe, output_dir):
    # Clear the output directory to prevent duplicate annotations
    clear_output_dir(output_dir)

    # Group the DataFrame by 'image_path'
    grouped = dataframe.groupby('image_path')
    tasks = [(image_path, group.to_dict('records'), output_dir) for image_path, group in grouped]

    # Use multiprocessing Pool to process tasks
    with multiprocessing.Pool() as pool:
        list(tqdm(pool.imap_unordered(save_yolo_annotations_task, tasks), total=len(tasks)))


# Save train and validation labels to their respective dirs
process_dataset(X_train, TRAIN_LABELS_DIR)
process_dataset(X_val, VAL_LABELS_DIR)


# Train images dir
TRAIN_IMAGES_DIR


# Create a data.yaml file required by yolo
class_names = sorted(train['class'].unique().tolist())
num_classes = len(class_names)

data_yaml = {
    'train': '/home/sagemaker-user/Ghana-Crop-Disease-Detection/' + str(TRAIN_IMAGES_DIR),
    'val': '/home/sagemaker-user/Ghana-Crop-Disease-Detection/' + str(VAL_IMAGES_DIR),
    'test': '/home/sagemaker-user/Ghana-Crop-Disease-Detection/' + str(TEST_IMAGES_DIR),
    'nc': num_classes,
    'names': class_names
}

yaml_path = 'data.yaml'
with open(yaml_path, 'w') as file:
    yaml.dump(data_yaml, file, default_flow_style=False)

# Preview data yaml file
data_yaml


# Plot some images and their bboxes to ensure the conversion was done correctly
def load_annotations(label_path):
    with open(label_path, 'r') as f:
        lines = f.readlines()
    boxes = []
    for line in lines:
        class_id, x_center, y_center, width, height = map(float, line.strip().split())
        boxes.append((class_id, x_center, y_center, width, height))
    return boxes

# Function to plot an image with its bounding boxes
def plot_image_with_boxes(image_path, boxes):
    # Load the image
    image = np.array(Image.open(str(image_path)))


    # Get image dimensions
    h, w, _ = image.shape

    # Plot the image
    plt.figure(figsize=(10, 10))
    plt.imshow(image)

    # Plot each bounding box
    for box in boxes:
        class_id, x_center, y_center, width, height = box
        # Convert YOLO format to corner coordinates
        xmin = int((x_center - width / 2) * w)
        ymin = int((y_center - height / 2) * h)
        xmax = int((x_center + width / 2) * w)
        ymax = int((y_center + height / 2) * h)

        # Draw the bounding box
        plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                          edgecolor='red', facecolor='none', linewidth=2))
        plt.text(xmin, ymin - 10, f'Class {int(class_id)}', color='red', fontsize=8, weight='bold')

    plt.axis('off')
    plt.show()

# Directories for images and labels
IMAGE_DIR = TRAIN_IMAGES_DIR
LABEL_DIR = TRAIN_LABELS_DIR

# Plot a few images with their annotations
for image_name in os.listdir(IMAGE_DIR)[:5]:
    image_path = IMAGE_DIR / image_name
    label_path = LABEL_DIR / (image_name.replace('.jpg', '.txt').replace('.png', '.txt'))

    if label_path.exists():
        boxes = load_annotations(label_path)
        print(f"Plotting {image_name} with {len(boxes)} bounding boxes.")
        plot_image_with_boxes(image_path, boxes)
    else:
        print(f"No annotations found for {image_name}.")






import torch
import torch.nn as nn
import torch.nn.functional as F

# Define Focus layer
class Focus(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):
        super(Focus, self).__init__()
        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size, stride, padding=(kernel_size // 2))
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = nn.SiLU()

    def forward(self, x):
        return self.act(self.bn(self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], dim=1))))

# Define C3k2 module
class C3k2(nn.Module):
    def __init__(self, in_channels, out_channels, n=1, shortcut=True, expansion=0.5):
        super(C3k2, self).__init__()
        hidden_channels = int(out_channels * expansion)
        self.conv1 = nn.Conv2d(in_channels, hidden_channels, 1, 1, 0)
        self.conv2 = nn.Conv2d(hidden_channels, out_channels, 3, 1, 1)
        self.bn1 = nn.BatchNorm2d(hidden_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.act = nn.SiLU()
        self.m = nn.Sequential(*[Bottleneck(hidden_channels, hidden_channels, shortcut) for _ in range(n)])

    def forward(self, x):
        x1 = self.act(self.bn1(self.conv1(x)))
        x1 = self.m(x1)
        return self.act(self.bn2(self.conv2(x1)))

# Bottleneck layer for shortcut connections
class Bottleneck(nn.Module):
    def __init__(self, in_channels, out_channels, shortcut=True):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, 1, 0)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.act = nn.SiLU()
        self.add = shortcut and in_channels == out_channels

    def forward(self, x):
        y = self.act(self.bn1(self.conv1(x)))
        y = self.act(self.bn2(self.conv2(y)))
        return y + x if self.add else y

# SPPF module
class SPPF(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=5):
        super(SPPF, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, 1, 0)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 1, kernel_size // 2, groups=out_channels)
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = nn.SiLU()

    def forward(self, x):
        y1 = self.conv1(x)
        y2 = self.conv2(y1)
        return self.act(self.bn(y1 + y2))

# Neck with FPN and PAN
class Neck(nn.Module):
    def __init__(self):
        super(Neck, self).__init__()
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.concat = lambda x, y: torch.cat([x, y], dim=1)

        self.c3_p4 = C3k2(1024 + 512, 512, 2, False)
        self.c3_p3 = C3k2(512 + 256, 256, 2, False)
        self.c3_p2 = C3k2(256 + 256, 256, 2, False)

        self.c3_n3 = C3k2(256 + 256, 256, 2, False)
        self.c3_n4 = C3k2(256 + 512, 512, 2, False)
        self.c3_n5 = C3k2(512 + 1024, 1024, 2, False)

    def forward(self, p5, p4, p3, p2):
        # FPN Top-down
        p5_up = self.upsample(p5)
        p4_fpn = self.c3_p4(self.concat(p5_up, p4))

        p4_up = self.upsample(p4_fpn)
        p3_fpn = self.c3_p3(self.concat(p4_up, p3))

        p3_up = self.upsample(p3_fpn)
        p2_fpn = self.c3_p2(self.concat(p3_up, p2))

        # PAN Bottom-up
        p2_down = F.max_pool2d(p2_fpn, 2)
        p3_pan = self.c3_n3(self.concat(p2_down, p3_fpn))

        p3_down = F.max_pool2d(p3_pan, 2)
        p4_pan = self.c3_n4(self.concat(p3_down, p4_fpn))

        p4_down = F.max_pool2d(p4_pan, 2)
        p5_pan = self.c3_n5(self.concat(p4_down, p5))

        return p2_fpn, p3_pan, p4_pan, p5_pan

# YOLO Model
class YOLO(nn.Module):
    def __init__(self, num_classes):
        super(YOLO, self).__init__()
        self.backbone = nn.ModuleList([
            Focus(3, 64),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            C3k2(128, 256, 2),
            nn.Conv2d(256, 512, 3, stride=2, padding=1),
            C3k2(512, 512, 2),
            nn.Conv2d(512, 1024, 3, stride=2, padding=1),
            SPPF(1024, 1024),
        ])

        self.neck = Neck()
        self.detect = nn.ModuleList([
            nn.Conv2d(256, num_classes * (5 + num_classes), 1),  # P2
            nn.Conv2d(256, num_classes * (5 + num_classes), 1),  # P3
            nn.Conv2d(512, num_classes * (5 + num_classes), 1),  # P4
            nn.Conv2d(1024, num_classes * (5 + num_classes), 1),  # P5
        ])

    def forward(self, x):
        # Backbone forward pass
        p2, p3, p4, p5 = None, None, None, x
        for i, layer in enumerate(self.backbone):
            p5 = layer(p5)
            if i == 2:
                p2 = p5
            elif i == 4:
                p3 = p5
            elif i == 5:
                p4 = p5

        # Neck forward pass
        p2_fpn, p3_pan, p4_pan, p5_pan = self.neck(p5, p4, p3, p2)

        # Detection heads
        outputs = [
            self.detect[0](p2_fpn),
            self.detect[1](p3_pan),
            self.detect[2](p4_pan),
            self.detect[3](p5_pan),
        ]

        return outputs



import torch
from torch import optim

# Load the model
model = YOLO(num_classes=10)  # Example for a dataset with 10 classes

# Load pre-trained weights
pretrained_path = "yolo11n.pt" # the pre-trained weights
state_dict = torch.load(pretrained_path)
model.load_state_dict(state_dict, strict=False)  # Allow mismatched final layers

# Freeze backbone layers
for param in model.backbone.parameters():
    param.requires_grad = False

# Define optimizer and learning rate scheduler
optimizer = optim.SGD(
    filter(lambda p: p.requires_grad, model.parameters()),  # Only update unfrozen layers
    lr=1e-3,
    momentum=0.9,
    weight_decay=5e-4
)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# Prepare dataset and dataloader
# Replace with your dataset class
from torch.utils.data import DataLoader
from your_dataset import YourDataset

train_dataset = YourDataset(root_dir="path/to/dataset", split="train")
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)

# Training loop
model.train()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

num_epochs = 30
for epoch in range(num_epochs):
    for batch_idx, (images, targets) in enumerate(train_loader):
        images, targets = images.to(device), targets.to(device)
        optimizer.zero_grad()

        outputs = model(images)
        loss = compute_loss(outputs, targets)  # Define your loss function
        loss.backward()
        optimizer.step()

    scheduler.step()
    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}")

# Save fine-tuned model
torch.save(model.state_dict(), "yolo_finetuned.pth")



# Load a yolo pre-trained model

model = YOLO('yolo11n.pt')

# Fine tune model to our data
model.train(
    data='data.yaml',          # Path to the dataset configuration
    epochs=100,                 # Number of epochs
    imgsz=1024,                # Image size (height, width)
    batch=8,                   # Batch size
    device=0,                  # Device to use (0 for the first GPU)
    patience=5)


# Validate the model on the validation set

model = YOLO('/home/sagemaker-user/Ghana-Crop-Disease-Detection/runs/detect/train2/weights/best.pt')
results = model.val(data='data.yaml')



# Validate the model on the validation set
model = YOLO('/home/sagemaker-user/Ghana-Crop-Disease-Detection/runs/detect/train2/weights/best.pt')
results = model.val()


# Load the trained YOLO model
model = YOLO('/home/sagemaker-user/Ghana-Crop-Disease-Detection/runs/detect/train2/weights/best.pt')

# Path to the test images directory
test_dir_path = '/home/sagemaker-user/Ghana-Crop-Disease-Detection/dataset/images/test'

# Get a list of all image files in the test directory
image_files = os.listdir(test_dir_path)

# Initialize an empty list to store the results for all images
# Default settings
default_bbox = [0, 0, 100, 100]  # Example default bounding box
default_class_id = -1  # Default class ID for "no detection"
default_class_name = "Corn_Cercospora_Leaf_Spot"  # Default class name
default_confidence = 1.0  # High confidence for the default case

all_data = []

# Iterate through each image in the directory
for image_file in tqdm(image_files):
    # Full path to the image
    img_path = os.path.join(test_dir_path, image_file)

    # Make predictions on the image
    results = model(img_path)
    
    # Initialize placeholders
    boxes = []
    classes = []
    confidences = []
    
    # If detections are made
    if results and len(results[0].boxes) > 0:
        # Extract bounding boxes, confidence scores, and class labels
        boxes = results[0].boxes.xyxy.tolist()  # Bounding boxes in xyxy format
        classes = results[0].boxes.cls.tolist()  # Class indices
        confidences = results[0].boxes.conf.tolist()  # Confidence scores
        # Class names dictionary
        names = results[0].names
    else:
        # Assign default prediction
        print("no detection")
        boxes = [default_bbox]
        classes = [default_class_id]
        confidences = [default_confidence]
        names = {default_class_id: default_class_name}  # Include the default class name in the names dictionary
    
    # Iterate through the results for this image
    for box, cls, conf in zip(boxes, classes, confidences):
        x1, y1, x2, y2 = box
        detected_class = names[int(cls)]  # Get the class name from the names dictionary

        # Add the result to the all_data list
        all_data.append({
            'Image_ID': image_file,
            'class': detected_class,
            'confidence': conf,
            'ymin': y1,
            'xmin': x1,
            'ymax': y2,
            'xmax': x2
        })

# Convert the list to a DataFrame for all images
sub = pd.DataFrame(all_data)


# check for missing image id from the sub compared to the initial submission
# no_pred = set(sub['Image_ID']).symmetric_difference(set(test['Image_ID']))

# assign it a default using the last prediction
len(sub['Image_ID'])
# default = sub.tail(1)

# default['Image_ID'] = _pred

# sub.concat(default)



sub.head()


sub['class'].value_counts()


# Create submission file to be uploaded to Zindi for scoring
sub.to_csv('benchmark_1_submission.csv', index = False)



